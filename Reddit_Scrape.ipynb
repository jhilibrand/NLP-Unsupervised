{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0390148a-75e3-42be-ba65-a63571301bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import psaw\n",
    "import psycopg2\n",
    "from  psaw import PushshiftAPI\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d8ce03e-12ff-42de-8446-e1ba4113ffad",
   "metadata": {},
   "outputs": [],
   "source": [
    "api = PushshiftAPI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc3f2765-cd82-40fb-b524-4c2492334770",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(\n",
    "    client_id=\"MtjmLCtDmwlKT8ryL_Mgog\",\n",
    "    client_secret=\"E2RKMDZtQvR9HSRGGOw-wnU7bsqZ0Q\",\n",
    "    password=\"Jch4494!!\",\n",
    "    user_agent=\"testscript by u/94_meowmeow\",\n",
    "    username=\"94_meowmeow\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5630527a-3503-4a7e-a57d-7e310d42e227",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replies_of(top_level_comment, count_comment, sub_entries_textblob, sub_entries_nltk):\n",
    "    if len(top_level_comment.replies) == 0:\n",
    "        count_comment = 0\n",
    "        return\n",
    "    else:\n",
    "        for num, comment in enumerate(top_level_comment.replies):\n",
    "            try:\n",
    "                count_comment += 1\n",
    "                #print('-' * count_comment, comment.body)\n",
    "                text_blob_sentiment(comment.body, sub_entries_textblob)\n",
    "                nltk_sentiment(comment.body, sub_entries_nltk)\n",
    "            except:\n",
    "                continue\n",
    "            replies_of(comment, count_comment, sub_entries_textblob,sub_entries_nltk)\n",
    "\n",
    "            #https://towardsdatascience.com/automate-sentiment-analysis-process-for-reddit-post-textblob-and-vader-8a79c269522f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15020010-c435-4e08-b7c4-a623486b4594",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replies_of_content(top_level_comment, count_comment):\n",
    "    if len(top_level_comment.replies) == 0:\n",
    "        count_comment = 0\n",
    "        return\n",
    "    else:\n",
    "        for num, comment in enumerate(top_level_comment.replies):\n",
    "            try:\n",
    "                count_comment += 1\n",
    "                #print('-' * count_comment, comment.body)\n",
    "            except:\n",
    "                continue\n",
    "            replies_of(comment, count_comment, sub_entries_textblob,sub_entries_nltk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58d5bd60-837a-417c-9e56-79247d33a1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wsb_scrape_all(start_epoch, end_epoch, limit_size):\n",
    "    #all_posts=[]\n",
    "    soup=[]\n",
    "    top_posts = list(api.search_submissions(after=start_epoch,before=end_epoch, subreddit='wallstreetbets', sort = 'desc', sort_type = 'score', limit=limit_size))\n",
    "    for submission in list(top_posts):\n",
    "            submission_comm = reddit.submission(id=submission.id)\n",
    "            #general_data = [submission.title, dt.datetime.fromtimestamp(submission.created_utc), submission.url]\n",
    "            #all_posts.append(general_data)\n",
    "            for count, top_level_comment in enumerate(submission_comm.comments):\n",
    "                count_comm = 0\n",
    "                try :\n",
    "                    soup.append((top_level_comment.body, dt.datetime.fromtimestamp(submission.created_utc)))\n",
    "                    replies_of_content(top_level_comment,\n",
    "                               count_comm)\n",
    "                except:\n",
    "                    continue    \n",
    "\n",
    "    return(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "303b7b7b-e5c9-43a2-8a85-394f6127ccfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/jenniferhilibrand/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from textblob import TextBlob\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "def nltk_sentiment(review, sub_entries_nltk):\n",
    "    vs = sia.polarity_scores(review)\n",
    "    if not vs['neg'] > 0.05:\n",
    "        if vs['pos'] - vs['neg'] > 0:\n",
    "            sub_entries_nltk['positive']=sub_entries_nltk['positive'] + 1\n",
    "            return 'Positive'\n",
    "        else:\n",
    "            sub_entries_nltk['neutral']=sub_entries_nltk['neutral'] + 1\n",
    "            return 'Neutral'\n",
    "    elif not vs['pos'] > 0.05:\n",
    "        if vs['pos'] - vs['neg'] <= 0:\n",
    "            sub_entries_nltk['negative']=sub_entries_nltk['negative'] + 1\n",
    "            return 'Negative'\n",
    "        else:\n",
    "            sub_entries_nltk['neutral']=sub_entries_nltk['neutral'] + 1\n",
    "            return 'Neutral'\n",
    "    else:\n",
    "        sub_entries_nltk['neutral']=sub_entries_nltk['neutral'] + 1\n",
    "        return 'Neutral'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4125d50f-2733-411a-b9bd-fc3ddc26cea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wsb_scrape_titles(start_epoch, end_epoch, limit_size):\n",
    "    overall_data=[]\n",
    "    top_posts = list(api.search_submissions(after=start_epoch,before=end_epoch, subreddit='wallstreetbets', sort = 'desc', sort_type = 'score', limit=limit_size))\n",
    "    for submission in list(top_posts):\n",
    "            general_data = [submission.title,dt.datetime.fromtimestamp(submission.created_utc), submission.url]\n",
    "            words = submission.title.split()\n",
    "            title_tickers=[]\n",
    "\n",
    "            content_tickers=[]\n",
    "            sub_entries_nltk = {'negative': 0, 'positive' : 0, 'neutral' : 0}\n",
    "\n",
    "            nltk_sentiment(submission.title, sub_entries_nltk)\n",
    "            submission_comm = reddit.submission(id=submission.id)\n",
    "            #timestamp = reddit.submission()\n",
    "            for count, top_level_comment in enumerate(submission_comm.comments):\n",
    "\n",
    "                count_comm = 0\n",
    "                try :\n",
    "\n",
    "                    nltk_sentiment(top_level_comment.body, sub_entries_nltk)\n",
    "                    replies_of(top_level_comment,\n",
    "                               count_comm,\n",
    "                               sub_entries_textblob,\n",
    "                               sub_entries_nltk)\n",
    "                    for i in top_level_comment.body.split():\n",
    "                        content_tickers.extend(extract_ticker(i))\n",
    "                except:\n",
    "                    continue\n",
    "            total_comments = sub_entries_nltk['negative']+sub_entries_nltk['neutral']+sub_entries_nltk['positive']\n",
    "\n",
    "\n",
    "            vader_pos_perc = sub_entries_nltk['positive']/total_comments\n",
    "            vader_neg_perc = sub_entries_nltk['negative']/total_comments\n",
    "\n",
    "            sentiment_data=[]\n",
    "            sentiment_data = [submission.title, sub_entries_nltk, total_comments, vader_neg_perc, vader_pos_perc, title_tickers, dt.datetime.fromtimestamp(submission.created_utc), submission.url]\n",
    "\n",
    "\n",
    "            overall_data.append(sentiment_data)\n",
    "                \n",
    "    \n",
    "    return(overall_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "541ade6f-aab2-43dc-a726-34fbb5b309bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nwith open('/Users/jenniferhilibrand/Metis/NLP/WSB_Jun_Soup.pickle', 'wb') as to_save:\\n    pickle.dump(soup_df3 ,to_save)\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime as dt\n",
    "start_date=dt.date(2021,6,1) \n",
    "end_date = dt.date(2021,6,30)\n",
    "#soup_df3= pd.DataFrame(wsb_scrape_all(start_date, end_date, 100))\n",
    "\n",
    "\n",
    "'''\n",
    "with open('/Users/jenniferhilibrand/Metis/NLP/WSB_Jun_Soup.pickle', 'wb') as to_save:\n",
    "    pickle.dump(soup_df3 ,to_save)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a9af9a57-ab77-41ff-a0b6-3f5aa9532642",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>dates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>COULDN‚ÄôT POSSIBLY BE HEDGE FUNDS üíéüëêüíé</td>\n",
       "      <td>2021-02-03 14:54:00</td>\n",
       "      <td>2021-02-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>My 102 year old grandmother bought GME in Augu...</td>\n",
       "      <td>2021-02-02 08:29:24</td>\n",
       "      <td>2021-02-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>who else bought at $250+</td>\n",
       "      <td>2021-02-04 12:56:48</td>\n",
       "      <td>2021-02-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Who of my 300 brethren's are still in ?</td>\n",
       "      <td>2021-02-03 03:54:46</td>\n",
       "      <td>2021-02-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>APES IN CONTROL</td>\n",
       "      <td>2021-02-02 13:01:05</td>\n",
       "      <td>2021-02-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Billionaire Investor Mark Cuban Says Reddit St...</td>\n",
       "      <td>2021-02-01 14:03:19</td>\n",
       "      <td>2021-02-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Me too</td>\n",
       "      <td>2021-01-31 20:11:26</td>\n",
       "      <td>2021-01-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Together no one can stop us !!!</td>\n",
       "      <td>2021-02-02 21:14:54</td>\n",
       "      <td>2021-02-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Spread the word! üíéü§≤</td>\n",
       "      <td>2021-01-31 22:14:44</td>\n",
       "      <td>2021-01-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>If this isn't a sign, I don't know what is...üöÄ...</td>\n",
       "      <td>2021-02-02 18:33:03</td>\n",
       "      <td>2021-02-02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text           timestamp  \\\n",
       "0               COULDN‚ÄôT POSSIBLY BE HEDGE FUNDS üíéüëêüíé 2021-02-03 14:54:00   \n",
       "1  My 102 year old grandmother bought GME in Augu... 2021-02-02 08:29:24   \n",
       "2                           who else bought at $250+ 2021-02-04 12:56:48   \n",
       "3            Who of my 300 brethren's are still in ? 2021-02-03 03:54:46   \n",
       "4                                    APES IN CONTROL 2021-02-02 13:01:05   \n",
       "5  Billionaire Investor Mark Cuban Says Reddit St... 2021-02-01 14:03:19   \n",
       "6                                             Me too 2021-01-31 20:11:26   \n",
       "7                    Together no one can stop us !!! 2021-02-02 21:14:54   \n",
       "8                                Spread the word! üíéü§≤ 2021-01-31 22:14:44   \n",
       "9  If this isn't a sign, I don't know what is...üöÄ... 2021-02-02 18:33:03   \n",
       "\n",
       "        dates  \n",
       "0  2021-02-03  \n",
       "1  2021-02-02  \n",
       "2  2021-02-04  \n",
       "3  2021-02-03  \n",
       "4  2021-02-02  \n",
       "5  2021-02-01  \n",
       "6  2021-01-31  \n",
       "7  2021-02-02  \n",
       "8  2021-01-31  \n",
       "9  2021-02-02  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime as dt\n",
    "start_date=dt.date(2021,1,1) \n",
    "end_date = dt.date(2021,6,30)\n",
    "submissions = list(api.search_submissions(after=start_date, before= end_date,\n",
    "                            subreddit='wallstreetbets', sort = 'desc', sort_type = 'score',\n",
    "                            filter=['url','author', 'title', 'subreddit'],\n",
    "                            limit=2000))\n",
    "\n",
    "titles=[]\n",
    "times=[]\n",
    "for submission in submissions: \n",
    "    times.append(dt.datetime.fromtimestamp(submission.created_utc))\n",
    "    titles.append(submission.title)\n",
    "\n",
    "titles_df = pd.DataFrame(titles)\n",
    "titles_df['timestamp']=times\n",
    "\n",
    "from datetime import datetime\n",
    "cols = ['text','timestamp']\n",
    "titles_df.columns = cols\n",
    "dates=[]\n",
    "for i in titles_df['timestamp']:\n",
    "    dates.append(dt.datetime.date(i))\n",
    "titles_df['dates']=dates\n",
    "\n",
    "titles_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f95fca85-3652-4ab3-a5b6-8d5ceac6115b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/jenniferhilibrand/Metis/NLP/WSB_titles_only.pickle', 'wb') as to_save:\n",
    "    pickle.dump(titles_df ,to_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af80b339-b771-4c3e-a1e9-6fdf3be72eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_soup = pd.DataFrame(wsb_scrape_titles(start_date, end_date, 2000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8428d1-9666-4963-b3c1-c7db039d08d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "cols = ['text','timestamp']\n",
    "titles_soup.columns = cols\n",
    "dates=[]\n",
    "for i in titles_soup['timestamp']:\n",
    "    dates.append(dt.datetime.date(i))\n",
    "titles_soup['dates']=dates\n",
    "\n",
    "titles_soup.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:metis] *",
   "language": "python",
   "name": "conda-env-metis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
